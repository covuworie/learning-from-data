{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from math import exp, sqrt\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimize function E(u, v) = (u$e^{v}$-2v$e^{-u}$)$^{2}$\n",
    "<br>\n",
    "Derivatives:\n",
    "<br>\n",
    "$\\frac{\\partial E}{\\partial u}$ = 2 (u$e^{v}$-2v$e^{-u}$)($e^{v}$+2v$e^{-u}$)\n",
    "<br>\n",
    "$\\frac{\\partial E}{\\partial v}$ = 2 (u$e^{v}$-2v$e^{-u}$)(u$e^{v}$-2$e^{-u}$)\n",
    "<br>\n",
    "Start at (u,v) = (1,1) and with learning rate $\\eta$=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error:\n",
    "    \"\"\" Error function class\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate(u, v):\n",
    "        \"\"\"Evaluate the function\n",
    "        Args:\n",
    "        u (float): u coordinate\n",
    "        v (float): v coordinate\n",
    "        Returns:\n",
    "        The function value (float)\n",
    "        \"\"\"\n",
    "        return (u*exp(v)-2*v*exp(-u))**2\n",
    "    \n",
    "    @staticmethod\n",
    "    def dEdu(u, v):\n",
    "        \"\"\"Evaluate the partial derivative dE/du of the function\n",
    "        Args:\n",
    "        u (float): u coordinate\n",
    "        v (float): v coordinate\n",
    "        Returns:\n",
    "        The partial derivative dE/du (float)\n",
    "        \"\"\"\n",
    "        return 2*(u*exp(v)-2*v*exp(-u))*(exp(v)+2*v*exp(-u))\n",
    "    \n",
    "    @staticmethod\n",
    "    def dEdv(u, v):\n",
    "        \"\"\"Evaluate the partial derivative dE/dv of the function\n",
    "        Args:\n",
    "        u (float): u coordinate\n",
    "        v (float): v coordinate\n",
    "        Returns:\n",
    "        The partial derivative dE/dv (float)\n",
    "        \"\"\"\n",
    "        return 2*(u*exp(v)-2*v*exp(-u))*(u*exp(v)-2*exp(-u))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(u, v):\n",
    "        \"\"\"Evaluate the gradient of the function\n",
    "        Args:\n",
    "        u (float): u coordinate\n",
    "        v (float): v coordinate\n",
    "        Returns:\n",
    "        The function gradient value (float)\n",
    "        \"\"\"\n",
    "        return np.array([Error.dEdu(u,v), Error.dEdv(u,v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    \"\"\"Gradient descent class\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def minimize(f, initial_guess=np.array([1.0, 1.0]), learning_rate = 0.1, conv_error=10**-14, max_iterations=20):\n",
    "        \"\"\"Perform gradient descent minimization\n",
    "        Args:\n",
    "        f (function): Error function to minimize\n",
    "        initial_guess (numpy ndarray): Initial guess\n",
    "        learning_rate (float): Learning rate\n",
    "        conv_error (float): Error function value below which convergence is assumed \n",
    "        max_iterations (int): Maximum number of iterations\n",
    "        Returns:\n",
    "        Tuple(error, error_val, num_iterations) (Tuple(numpy ndarray, float, int): Tuple of error, \n",
    "        error function value at minimum, number of iterations\n",
    "        \"\"\"\n",
    "        prev_error = np.array(initial_guess) \n",
    "        num_iterations = 0\n",
    "        for i in range(0, max_iterations):\n",
    "            error = prev_error - learning_rate * f.gradient(prev_error[0], prev_error[1])\n",
    "            error_val = Error.evaluate(error[0], error[1])\n",
    "            prev_error =  error\n",
    "            num_iterations += 1\n",
    "            if error_val < conv_error:\n",
    "                break\n",
    "        return error, error_val, num_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final (u, v) =  [0.04473629 0.02395871]\n",
      "Number of iterations needed =  10\n"
     ]
    }
   ],
   "source": [
    "error, error_val, num_iterations = GradientDescent.minimize(Error)\n",
    "print('Final (u, v) = ', error)\n",
    "print('Number of iterations needed = ', num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoordinateDescent:\n",
    "    \"\"\"Coordinate descent class\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def minimize(f, initial_guess=np.array([1.0, 1.0]), learning_rate = 0.1, max_iterations=15):\n",
    "        \"\"\"Perform coordinate descent minimization\n",
    "        Args:\n",
    "        f (function): Error function to minimize\n",
    "        initial_guess (numpy ndarray): Initial guess\n",
    "        learning_rate (float): Learning rate\n",
    "        max_iterations (int): Maximum number of iterations\n",
    "        Returns:\n",
    "        Tuple(error, error_val) (Tuple(numpy ndarray, float): Tuple of error, error function value at minimum\n",
    "        \"\"\"\n",
    "        prev_error = np.array(initial_guess) \n",
    "        error = np.zeros(2, dtype=float)\n",
    "        for i in range(0, max_iterations):\n",
    "            error[0] = prev_error[0] - learning_rate * f.dEdu(prev_error[0], prev_error[1]) # 1st step\n",
    "            prev_error[0] = error[0]\n",
    "            error[1] = prev_error[1] - learning_rate * f.dEdv(prev_error[0], prev_error[1]) # 2nd step\n",
    "            prev_error[1] = error[1]\n",
    "        error_val = Error.evaluate(error[0], error[1])\n",
    "        return prev_error, error_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final (u, v) =  [ 6.2970759  -2.85230695]\n",
      "Final error =  0.13981379199615315\n"
     ]
    }
   ],
   "source": [
    "error, error_val = CoordinateDescent.minimize(Error)\n",
    "print('Final (u, v) = ', error)\n",
    "print('Final error = ', error_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point:\n",
    "    \"\"\" Point class for representing and manipulating random 2-dimensional coordinates. \"\"\"\n",
    "    \n",
    "    def __init__(self, lower=-1.0, upper=1.0):\n",
    "        \"\"\" Create a new random point assuming a uniform distribution between the upper and lower bounds. \n",
    "        Args:\n",
    "        lower (float): Lower bound.\n",
    "        upper (float): Upper bound.\n",
    "        \"\"\"\n",
    "        self.lower = lower # lower bound\n",
    "        self.upper = upper # upper bound\n",
    "        self.coordinates = np.random.uniform(lower, upper, 2) # point (numpy.ndarray) (half open interval!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetFunction:\n",
    "    \"\"\"TargetFunction class for representing and manipulation random lines in 2-dimensional coordinates.\"\"\"\n",
    "    def __init__(self, lower=-1.0, upper=1.0):\n",
    "        \"\"\" Create a new random line assuming a uniform distribution between the upper and lower bounds. \n",
    "        Args:\n",
    "        lower (float): Lower bound.\n",
    "        upper (float): Upper bound.\n",
    "        \"\"\"\n",
    "        self.points = [Point(lower, upper) for i in range(0,2)]\n",
    "        \n",
    "    def sign(self, point):\n",
    "        return np.sign(self.evaluate(point))\n",
    "        \n",
    "    def evaluate(self, point): \n",
    "        x_minus_x1  = point.coordinates[0] - self.points[0].coordinates[0]\n",
    "        y_minus_y1  = point.coordinates[1] - self.points[0].coordinates[1]\n",
    "        y2_minus_y1 = self.points[1].coordinates[1] - self.points[0].coordinates[1]\n",
    "        x2_minus_x1 = self.points[1].coordinates[0] - self.points[0].coordinates[0]\n",
    "        return x_minus_x1 * y2_minus_y1 - y_minus_y1 * x2_minus_x1\n",
    "    \n",
    "    def weights(self):\n",
    "        y1_times_x2 = self.points[0].coordinates[1] * self.points[1].coordinates[0]\n",
    "        x1_times_y2 = self.points[0].coordinates[0] * self.points[1].coordinates[1]\n",
    "        y2_minus_y1 = self.points[1].coordinates[1] - self.points[0].coordinates[1]\n",
    "        x2_minus_x1 = self.points[1].coordinates[0] - self.points[0].coordinates[0]   \n",
    "        return np.array([y1_times_x2 - x1_times_y2, y2_minus_y1, -x2_minus_x1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    \"\"\" DataSet class. \"\"\"\n",
    "    def __init__(self, N=2, lower=-1.0, upper=1.0):\n",
    "        \"\"\" Create a new data set assuming a uniform distribution between the \n",
    "            upper and lower bounds. \n",
    "        Args:\n",
    "        N (int): Number of points.\n",
    "        lower (float): Lower bound.\n",
    "        upper (float): Upper bound.\n",
    "        \"\"\"\n",
    "        self.inputs = [Point(lower, upper) for i in range(0,N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\" Logistic Regression class. \"\"\"\n",
    "    \n",
    "    def __init__(self, training_set):\n",
    "        \"\"\" Create a new Logistic Regression. \n",
    "        Args:\n",
    "        training_set (DataSet): Training set.\n",
    "        \"\"\"\n",
    "        self.training_set = training_set\n",
    "        \n",
    "    def run(self, target_function, initial_weights=np.zeros(3, dtype=float), learning_rate=0.01, tol=0.01, max_epochs=1000):    \n",
    "        \"\"\"Run of Logistic Regression Algorithm using Stochstic Gradient Descent (SGD) to minimize the in-sample error.\n",
    "        Args:\n",
    "        target_function (TargetFunction): Target function\n",
    "        initial_weights (np.ndarray): Initial guess for weight vector\n",
    "        learning_rate (float): Learning rate\n",
    "        tol (float): Tolerance in weight vector for which convergence is assumed\n",
    "        max_epochs (int): Maximum number of epochs\n",
    "        Returns:\n",
    "        Tuple(g, num_epochs)\n",
    "        \"\"\"\n",
    "        # set inputs and outputs\n",
    "        x = self.transformPoints(self.training_set.inputs) # transform inputs\n",
    "        y = np.array([target_function.sign(input) for input in self.training_set.inputs], copy=True)\n",
    "        data = list(zip(x, y))\n",
    "        \n",
    "        # initialize weight vector\n",
    "        w_prev_epoch = np.array(initial_weights)\n",
    "        w_prev = w_prev_epoch\n",
    "        \n",
    "        num_epochs = 0\n",
    "        for t in range(0, max_epochs):   \n",
    "            num_epochs += 1\n",
    "            # randomly permute the data points\n",
    "            data_epoch = np.random.permutation(data)\n",
    "            # use Stochastic Gradient Descent (SGD) to update the weight vector for each training example \n",
    "            for example in data_epoch:\n",
    "                xn = example[0]\n",
    "                yn = example[1]\n",
    "                gradient_Ein = -yn * xn / (1. + np.exp(yn * np.dot(w_prev.T, xn)))\n",
    "                w = w_prev - learning_rate * gradient_Ein\n",
    "                w_prev = w    \n",
    "            \n",
    "            w_epoch = w\n",
    "            #print('norm(w_prev_epoch - w_epoch) = ', norm(w_prev_epoch - w_epoch))\n",
    "            if norm(w_prev_epoch - w_epoch) < tol:\n",
    "                break\n",
    "            w_prev_epoch = w_epoch\n",
    "            \n",
    "        return w_epoch, num_epochs\n",
    "    \n",
    "    def Eout(self, weights, target_function, n_points=1000):\n",
    "        \"\"\"Evaluate the cross-entropy error (Eout)\n",
    "        Args:\n",
    "        weights (np.ndarray): g function\n",
    "        target_function (TargetFunction): Target function\n",
    "        n_points (int): Number of points to generate\n",
    "        Returns:\n",
    "        Cross-entropy error (Eout)\n",
    "        \"\"\"\n",
    "        # generate random points    \n",
    "        estPoints = [Point(-1.0, 1.0) for i in range(0, n_points)]\n",
    "        \n",
    "        # compute inputs and outputs\n",
    "        x = self.transformPoints(estPoints)\n",
    "        y = np.array([target_function.sign(estPoint) for estPoint in estPoints], copy=True)\n",
    "        \n",
    "        # compute Eout\n",
    "        eouts = []\n",
    "        for i in range(0, len(x)):\n",
    "            xn = x[i]\n",
    "            yn = y[i]\n",
    "            eouts.append(np.log(1. + np.exp(-yn * np.dot(weights.T, xn))))\n",
    "        eout = np.mean(eouts)\n",
    "        return eout\n",
    "         \n",
    "    # privates\n",
    "    def transformPoints(self, points):\n",
    "        return np.array([np.insert(pnt.coordinates, 0, 1.0) for pnt in points], copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eout =  0.10345886899317966\n",
      "Number of epochs for convergence =  341.92\n"
     ]
    }
   ],
   "source": [
    "# Create training set\n",
    "N = 100\n",
    "training_set = DataSet(N)\n",
    "\n",
    "# Run logistic regression\n",
    "num_runs = 100\n",
    "eouts = []\n",
    "n_epochs = []\n",
    "for i in range(0, num_runs):\n",
    "    lr = LogisticRegression(training_set) \n",
    "    target_function = TargetFunction()\n",
    "    g, num_epochs = lr.run(target_function)\n",
    "    eouts.append(lr.Eout(g, target_function))\n",
    "    n_epochs.append(num_epochs)\n",
    "\n",
    "eout = np.mean(eouts)\n",
    "num_epoch = np.mean(n_epochs)\n",
    "print('Eout = ', eout)\n",
    "print('Number of epochs for convergence = ', num_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
